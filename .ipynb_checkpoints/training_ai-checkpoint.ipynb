{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#differant biometrics in this order height(cm),wingspan(cm), weight(kg),gender(0 male 1 female),age(years)\n",
    "# biometric = [185, 184,82,0,23]\n",
    "\n",
    "LEARNING_RATE = 0.2\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 1000\n",
    "SHOW_EVERY = 50\n",
    "#training proportions\n",
    "# observation={'rest':np.random.randint(100),'ut3':np.random.randint(100),'ut2':np.random.randint(100),'ut1':np.random.randint(100),'at':np.random.randint(100),'an':np.random.randint(100)}\n",
    "#The percentages of training and rest\n",
    "intensities={'training':50,'rest':50}\n",
    "#labels of how much speed(in m/s) increase/decrease there was in each training camp\n",
    "rewards=-(np.ones(100))\n",
    "for i in range (70,76):\n",
    "    rewards[i]=1\n",
    "for i in range (55,70):\n",
    "    rewards[i]=.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      " -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      " -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      " -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.   0.5\n",
      "  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5\n",
      "  1.   1.   1.   1.   1.   1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      " -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      " -1.  -1. ]\n"
     ]
    }
   ],
   "source": [
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation=list()\n",
    "for i in range(101):\n",
    "    observation.append([i,100-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every possible percentages\n",
    "q_table=np.random.uniform(low=-2, high=0, size=(101,101,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Andrew set this up so it needs to take the observation as state, Not sure what the right answer would be but it\n",
    "#updates the observation\n",
    "def do_action(state, action,episode):\n",
    "    \n",
    "    #do nothing\n",
    "    if action==0:\n",
    "        if episode % SHOW_EVERY==0:\n",
    "            print(f\"Intensities are not changed, {intensities}\")\n",
    "        reward=rewards[state[0]]\n",
    "        return state, reward\n",
    "\n",
    "    #increase training,decrease rest\n",
    "    elif action==1:\n",
    "        intensities['training']+=10\n",
    "        intensities['rest']-=10\n",
    "        new_state=(state[0]+10,state[1]-10)\n",
    "        reward=rewards[state[0]]\n",
    "        if episode % SHOW_EVERY==0:\n",
    "            print(f\"Training is increased  {intensities}\")\n",
    "        return new_state, reward \n",
    "        \n",
    "    #decrease training, increase rest\n",
    "    elif action==2:\n",
    "        intensities['training']-=10\n",
    "        intensities['rest']+=10\n",
    "        new_state=(state[0]-10,state[1]+10)\n",
    "        reward=rewards[state[0]]\n",
    "        if episode % SHOW_EVERY==0:\n",
    "            print(f\"Rest is increased  {intensities}\")\n",
    "        return new_state, reward\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Training is increased  {'training': 60, 'rest': 40}\n",
      "Optimal training reached {'training': 60, 'rest': 40}\n"
     ]
    }
   ],
   "source": [
    "state=tuple(observation[50])\n",
    "for episode in range (EPISODES):\n",
    "    action=np.argmax(q_table[state])\n",
    "    if episode % SHOW_EVERY==0:\n",
    "        print(action)\n",
    "    state, reward=do_action(state, action,episode)\n",
    "    max_future_q = np.max(q_table[state])\n",
    "    current_q = q_table[state + (action,)]\n",
    "    new_q=(1-LEARNING_RATE)*current_q+LEARNING_RATE*(reward+DISCOUNT*max_future_q)\n",
    "    q_table[state + (action,)] = new_q\n",
    "    if reward ==1:\n",
    "        print(f\"Optimal training reached {intensities}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.52912533,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env= gym.make(\"MountainCar-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.52808386,  0.00104145], dtype=float32), -1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "print(env.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(env.observation_space.high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 20]\n"
     ]
    }
   ],
   "source": [
    "DISCRETE_OS_SIZE= [20]* len(env.observation_space.high)\n",
    "print(DISCRETE_OS_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_win_size= (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09  0.007]\n"
     ]
    }
   ],
   "source": [
    "print(discrete_win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2,high=0,size=(DISCRETE_OS_SIZE+[env.action_space.n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low) / discrete_win_size\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46534818  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_state = get_discrete_state(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 10)\n"
     ]
    }
   ],
   "source": [
    "print(discrete_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.94716143, -1.14124258, -0.760841  ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[discrete_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(q_table[discrete_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.48570472, -0.00129323], dtype=float32), -1.0, False, {})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, done, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48828155, -0.00257682], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 9)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_discrete_state(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\my pc\\anaconda3\\envs\\AI\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "Objective reached on episode 303\n",
      "reward is -1.0\n",
      "Objective reached on episode 332\n",
      "reward is -1.0\n",
      "Objective reached on episode 334\n",
      "reward is -1.0\n",
      "Objective reached on episode 342\n",
      "reward is -1.0\n",
      "350\n",
      "Objective reached on episode 374\n",
      "reward is -1.0\n",
      "Objective reached on episode 377\n",
      "reward is -1.0\n",
      "Objective reached on episode 393\n",
      "reward is -1.0\n",
      "400\n",
      "Objective reached on episode 401\n",
      "reward is -1.0\n",
      "Objective reached on episode 402\n",
      "reward is -1.0\n",
      "Objective reached on episode 408\n",
      "reward is -1.0\n",
      "Objective reached on episode 414\n",
      "reward is -1.0\n",
      "Objective reached on episode 415\n",
      "reward is -1.0\n",
      "Objective reached on episode 416\n",
      "reward is -1.0\n",
      "Objective reached on episode 417\n",
      "reward is -1.0\n",
      "Objective reached on episode 418\n",
      "reward is -1.0\n",
      "Objective reached on episode 431\n",
      "reward is -1.0\n",
      "Objective reached on episode 435\n",
      "reward is -1.0\n",
      "450\n",
      "Objective reached on episode 460\n",
      "reward is -1.0\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Wrapper.close of <TimeLimit<MountainCarEnv<MountainCar-v0>>>>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for episode in range(EPISODES):0.40797842, -0.003766\n",
    "    if episode % SHOW_EVERY ==0:\n",
    "        print(episode)\n",
    "        render = True\n",
    "    else:\n",
    "        render = False\n",
    "    discrete_state= get_discrete_state(env.reset())\n",
    "    done= False\n",
    "\n",
    "    while not done:\n",
    "        #Find the position of the highest value\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        \n",
    "        #\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        if reward != -1:\n",
    "            print(f\"the reward is {reward} episode {episode}\")\n",
    "#         print(f\"new_state: {new_state} reward: {reward} done: {done} _:{_}\")\n",
    "\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        if not done:\n",
    "            max_future_q = np.amax(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state +(action, )]\n",
    "\n",
    "            new_q = (1-LEARNING_RATE) * current_q + LEARNING_RATE + (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state+(action, )] = new_q\n",
    "\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            print(f\"Objective reached on episode {episode}\")\n",
    "            print(f\"reward is {reward}\")\n",
    "            env.render()\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state=new_discrete_state\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
