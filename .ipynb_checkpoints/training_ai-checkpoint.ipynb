{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#differant biometrics in this order height(cm),wingspan(cm), weight(kg),gender(0 male 1 female),age(years)\n",
    "# biometric = [185, 184,82,0,23]\n",
    "\n",
    "LEARNING_RATE = 0.2\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 2000\n",
    "SHOW_EVERY = 50\n",
    "#training proportions\n",
    "# observation={'rest':np.random.randint(100),'ut3':np.random.randint(100),'ut2':np.random.randint(100),'ut1':np.random.randint(100),'at':np.random.randint(100),'an':np.random.randint(100)}\n",
    "\n",
    "#The percentages of training and rest\n",
    "intensities={'training':50,'rest':50}\n",
    "\n",
    "#labels of what percentages are desirable. This is a simlified version of improvement.\n",
    "#If bot requests the athlete to train between 70-75% training, then it wins\n",
    "rewards=-(np.ones(100))\n",
    "for i in range (70,76):\n",
    "    rewards[i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this prints all possible training percentages 0-100% with the corresponding rest 100-0%\n",
    "observation=list()\n",
    "for i in range(101):\n",
    "    observation.append([i,100-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a map of whether to increase,decrease or neither to the intensities.\n",
    "q_table=np.random.uniform(low=-1, high=0, size=(101,101,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_action(state, action,episode):\n",
    "    \n",
    "    #increase training,decrease rest\n",
    "    if action==1 and intensities['training']<100:\n",
    "        intensities['training']+=1\n",
    "        intensities['rest']-=1\n",
    "        new_state=(state[0]+1,state[1]-1)\n",
    "        reward=rewards[state[0]]\n",
    "        if episode % SHOW_EVERY==0:\n",
    "            print(f\"Training is increased  {intensities}\")\n",
    "        return new_state, reward \n",
    "        \n",
    "    #decrease training, increase rest\n",
    "    elif action==2 and intensities['rest']<100:\n",
    "        intensities['training']-=1\n",
    "        intensities['rest']+=1\n",
    "        new_state=(state[0]-1,state[1]+1)\n",
    "        reward=rewards[state[0]]\n",
    "        if episode % SHOW_EVERY==0:\n",
    "            print(f\"Rest is increased  {intensities}\")\n",
    "        return new_state, reward\n",
    "    \n",
    "        #do nothing\n",
    "    else:\n",
    "        if episode % SHOW_EVERY==0:\n",
    "            print(f\"Intensities are not changed, {intensities}\")\n",
    "        reward=rewards[state[0]]\n",
    "        return state, reward\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is increased  {'training': 51, 'rest': 49}\n",
      "Episode: 0\n",
      "Rest is increased  {'training': 55, 'rest': 45}\n",
      "Episode: 50\n",
      "Optimal training reached {'training': 70, 'rest': 30} by episode 92\n"
     ]
    }
   ],
   "source": [
    "#For loop\n",
    "state=tuple(observation[50])\n",
    "for episode in range (EPISODES):\n",
    "    action=np.argmax(q_table[state])\n",
    "    new_state, reward=do_action(state, action,episode)\n",
    "    max_future_q = np.amax(q_table[new_state])\n",
    "    current_q = q_table[state +(action, )]\n",
    "    new_q = (1-LEARNING_RATE) * current_q + LEARNING_RATE + (reward + DISCOUNT * max_future_q)\n",
    "    q_table[state+(action, )] = new_q\n",
    "    state=new_state\n",
    "    if episode % SHOW_EVERY==0:\n",
    "        print(f\"Episode: {episode}\")\n",
    "    if reward ==1:\n",
    "        print(f\"Optimal training reached {intensities} by episode {episode}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env= gym.make(\"MountainCar-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(env.observation_space.high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCRETE_OS_SIZE= [20]* len(env.observation_space.high)\n",
    "print(DISCRETE_OS_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_win_size= (env.observation_space.high - env.observation_space.low) / DISCRETE_OS_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(discrete_win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.uniform(low=-2,high=0,size=(DISCRETE_OS_SIZE+[env.action_space.n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low) / discrete_win_size\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_state = get_discrete_state(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(discrete_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table[discrete_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(q_table[discrete_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, done, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_discrete_state(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for episode in range(EPISODES):\n",
    "    if episode % SHOW_EVERY ==0:\n",
    "        print(episode)\n",
    "        render = True\n",
    "    else:\n",
    "        render = False\n",
    "    discrete_state= get_discrete_state(env.reset())\n",
    "    done= False\n",
    "\n",
    "    while not done:\n",
    "        #Find the position of the highest value\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        \n",
    "        #\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        if reward != -1:\n",
    "            print(f\"the reward is {reward} episode {episode}\")\n",
    "#         print(f\"new_state: {new_state} reward: {reward} done: {done} _:{_}\")\n",
    "\n",
    "\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        if not done:\n",
    "            max_future_q = np.amax(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state +(action, )]\n",
    "\n",
    "            new_q = (1-LEARNING_RATE) * current_q + LEARNING_RATE + (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state+(action, )] = new_q\n",
    "\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            print(f\"Objective reached on episode {episode}\")\n",
    "            print(f\"reward is {reward}\")\n",
    "            env.render()\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state=new_discrete_state\n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
